# -*- coding: utf-8 -*-
"""terafac-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CyRO1m5omOKuYPc1siOxaLz5EeSKOZGc
"""

!pip install timm -q

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
import timm
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import os
from google.colab import drive

# Mount drive to save checkpoints
drive.mount('/content/drive')
save_dir = '/content/drive/MyDrive/CIFAR10_Challenge'
os.makedirs(save_dir, exist_ok=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Set seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# For Level 3, I'm using a balanced augmentation approach
# Not as heavy as Level 2, but still effective

train_transforms = transforms.Compose([
    transforms.Resize(144),  # Slightly larger than Level 1 & 2
    transforms.RandomCrop(144, padding=18),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    transforms.RandomErasing(p=0.3)
])

test_transforms = transforms.Compose([
    transforms.Resize(144),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

print("Loading CIFAR-10 dataset...")
train_dataset = CIFAR10(root='./data', train=True, download=True, transform=train_transforms)
test_dataset = CIFAR10(root='./data', train=False, download=True, transform=test_transforms)

# Split into 80-10-10 as required
train_set, val_set = random_split(train_dataset, [45000, 5000],
                                   generator=torch.Generator().manual_seed(42))
test_set = torch.utils.data.Subset(test_dataset, range(5000))

print(f"Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)}")

# Create dataloaders
batch_size = 128

train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,
                          num_workers=2, pin_memory=True, persistent_workers=True)
val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,
                        num_workers=2, pin_memory=True, persistent_workers=True)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False,
                         num_workers=2, pin_memory=True)

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

print("\nInitializing EfficientNet-B2...")
print("EfficientNet uses compound scaling (depth, width, resolution)")
print("B2 is a good balance between speed and accuracy for our task")

# Load pretrained EfficientNet-B2
model = timm.create_model('efficientnet_b2', pretrained=True, num_classes=10)
model = model.to(device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nTotal parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# Mixed precision for faster training
scaler = torch.amp.GradScaler('cuda')

# Using cross entropy with label smoothing
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

# I'll use a two-phase training approach:
# Phase 1: Train only the classifier head (faster convergence)
# Phase 2: Fine-tune the entire model (better final accuracy)

print("\n" + "="*70)
print("Training Strategy:")
print("Phase 1: Train classifier only (5 epochs)")
print("Phase 2: Fine-tune entire model (20 epochs)")
print("="*70)

# Phase 1: Freeze backbone, train classifier
for name, param in model.named_parameters():
    if 'classifier' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                        lr=1e-3, weight_decay=0.01)

num_epochs_phase1 = 5
num_epochs_phase2 = 20

scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs_phase1)

# Track training history
history = {
    'train_loss': [],
    'train_acc': [],
    'val_loss': [],
    'val_acc': [],
    'lr': []
}

def train_one_epoch(model, loader, criterion, optimizer, scaler):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(loader, desc='Training')
    for inputs, targets in pbar:
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()

        # Mixed precision training
        with torch.amp.autocast('cuda'):
            outputs = model(inputs)
            loss = criterion(outputs, targets)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

        # Update progress bar
        pbar.set_postfix({
            'loss': f'{loss.item():.3f}',
            'acc': f'{100.*correct/total:.2f}%'
        })

    epoch_loss = running_loss / len(loader)
    epoch_acc = correct / total
    return epoch_loss, epoch_acc

@torch.no_grad()
def evaluate(model, loader, criterion):
    """Evaluate the model"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, targets in tqdm(loader, desc='Validation'):
        inputs, targets = inputs.to(device), targets.to(device)

        with torch.amp.autocast('cuda'):
            outputs = model(inputs)
            loss = criterion(outputs, targets)

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    epoch_loss = running_loss / len(loader)
    epoch_acc = correct / total
    return epoch_loss, epoch_acc

print("\n" + "="*70)
print("PHASE 1: Training classifier head")
print("="*70)

best_val_acc = 0.0

for epoch in range(num_epochs_phase1):
    print(f"\nEpoch {epoch+1}/{num_epochs_phase1}")

    # Train
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion,
                                            optimizer, scaler)

    # Validate
    val_loss, val_acc = evaluate(model, val_loader, criterion)

    # Update learning rate
    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']

    # Save history
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['lr'].append(current_lr)

    print(f"Train: Loss={train_loss:.4f}, Acc={train_acc*100:.2f}%")
    print(f"Val:   Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc
        }, f'{save_dir}/level3_best_model.pth')
        print(f"✓ Best model saved! Val accuracy: {val_acc*100:.2f}%")

print("\n" + "="*70)
print("PHASE 2: Fine-tuning entire model")
print("="*70)

# Unfreeze all layers
for param in model.parameters():
    param.requires_grad = True

# Use lower learning rate for fine-tuning
optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs_phase2)

# Early stopping
patience = 7
patience_counter = 0

for epoch in range(num_epochs_phase2):
    print(f"\nEpoch {num_epochs_phase1 + epoch + 1}/{num_epochs_phase1 + num_epochs_phase2}")

    # Train
    train_loss, train_acc = train_one_epoch(model, train_loader, criterion,
                                            optimizer, scaler)

    # Validate
    val_loss, val_acc = evaluate(model, val_loader, criterion)

    # Update learning rate
    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']

    # Save history
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['lr'].append(current_lr)

    print(f"Train: Loss={train_loss:.4f}, Acc={train_acc*100:.2f}%")
    print(f"Val:   Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%")

    # Save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        torch.save({
            'epoch': num_epochs_phase1 + epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc
        }, f'{save_dir}/level3_best_model.pth')
        print(f"✓ Best model saved! Val accuracy: {val_acc*100:.2f}%")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"\nEarly stopping triggered after {patience} epochs without improvement")
            break

print("\n" + "="*70)
print("Loading best model for final evaluation...")
print("="*70)

checkpoint = torch.load(f'{save_dir}/level3_best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])

test_loss, test_acc = evaluate(model, test_loader, criterion)

print("\n" + "="*70)
print("LEVEL 3 FINAL RESULTS")
print("="*70)
print(f"Model: EfficientNet-B2")
print(f"Test Accuracy: {test_acc*100:.2f}%")
print(f"Best Val Acc:  {best_val_acc*100:.2f}%")
print(f"\nPrevious Levels:")
print(f"  Level 1 (ResNet50):          96.34%")
print(f"  Level 2 (ResNet50 + Aug):    96.88%")
print(f"  Level 3 (EfficientNet-B2):   {test_acc*100:.2f}%")
print(f"\nTarget: 91-93%+")
print(f"Status: {'✓ PASSED' if test_acc >= 0.91 else '✗ NEEDS WORK'}")
print("="*70)

# Plot training curves
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss curve
axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2, color='blue')
axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, color='orange')
axes[0].axvline(x=num_epochs_phase1, color='red', linestyle='--',
                label='Start Fine-tuning', alpha=0.6)
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Training and Validation Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Accuracy curve
axes[1].plot([acc*100 for acc in history['train_acc']],
             label='Train Acc', linewidth=2, color='blue')
axes[1].plot([acc*100 for acc in history['val_acc']],
             label='Val Acc', linewidth=2, color='orange')
axes[1].axvline(x=num_epochs_phase1, color='red', linestyle='--',
                label='Start Fine-tuning', alpha=0.6)
axes[1].axhline(y=91, color='green', linestyle='--',
                label='Target (91%)', alpha=0.6)
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy (%)')
axes[1].set_title('Training and Validation Accuracy')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f'{save_dir}/level3_training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"\n✓ Training curves saved")

print("\nAnalyzing per-class performance...")

class_correct = [0] * 10
class_total = [0] * 10

model.eval()
with torch.no_grad():
    for inputs, targets in test_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        _, predicted = outputs.max(1)

        for i in range(len(targets)):
            label = targets[i].item()
            class_correct[label] += (predicted[i] == targets[i]).item()
            class_total[label] += 1

print("\nPer-class accuracy:")
for i in range(10):
    acc = 100 * class_correct[i] / class_total[i]
    print(f"  {class_names[i]:12s}: {acc:5.2f}%")

# Visualize per-class performance
plt.figure(figsize=(10, 6))
class_accuracies = [100 * class_correct[i] / class_total[i] for i in range(10)]
colors = ['green' if acc >= 95 else 'orange' if acc >= 90 else 'red'
          for acc in class_accuracies]
bars = plt.bar(class_names, class_accuracies, color=colors, alpha=0.7, edgecolor='black')

plt.axhline(y=91, color='red', linestyle='--', label='Target (91%)', alpha=0.7)
plt.xlabel('Class')
plt.ylabel('Accuracy (%)')
plt.title('Level 3: Per-Class Test Accuracy (EfficientNet-B2)')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 100)
plt.legend()
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig(f'{save_dir}/level3_per_class_accuracy.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"✓ Per-class accuracy chart saved")

summary = f"""
LEVEL 3 - COMPLETION SUMMARY
{'='*70}

Architecture: EfficientNet-B2
Key Innovation: Using compound scaling and efficient convolutions
Training Strategy: Two-phase (classifier → full fine-tuning)

Dataset: CIFAR-10
Split: 80% train / 10% val / 10% test

RESULTS:
  Test Accuracy:  {test_acc*100:.2f}%
  Best Val Acc:   {best_val_acc*100:.2f}%
  Total Epochs:   {len(history['train_acc'])}

COMPARISON WITH PREVIOUS LEVELS:
  Level 1 (ResNet50 baseline):     96.34%
  Level 2 (ResNet50 + aug):        96.88%
  Level 3 (EfficientNet-B2):       {test_acc*100:.2f}%

TARGET: 91-93%+
STATUS: {'PASSED ✓' if test_acc >= 0.91 else 'NEEDS IMPROVEMENT'}

WHY EFFICIENTNET-B2?
- Compound scaling balances depth, width, and resolution
- More parameter-efficient than ResNet
- Better accuracy-to-computation ratio
- Proven strong performance on image classification

OBSERVATIONS:
- EfficientNet converges {'faster' if test_acc > 0.965 else 'similarly'} compared to ResNet50
- Per-class performance is {'more balanced' if min(class_accuracies) > 90 else 'comparable'}
- Model size: {total_params:,} parameters

FILES SAVED:
- {save_dir}/level3_best_model.pth
- {save_dir}/level3_training_curves.png
- {save_dir}/level3_per_class_accuracy.png


{'='*70}
"""

with open(f'{save_dir}/level3_summary.txt', 'w') as f:
    f.write(summary)

print(summary)
print("\n" + "="*70)
print("✓✓✓ LEVEL 3 COMPLETE! ✓✓✓")
print("="*70)